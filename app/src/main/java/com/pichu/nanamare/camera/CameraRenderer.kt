package com.pichu.nanamare.camera

import android.annotation.SuppressLint
import android.content.Context
import android.content.res.Configuration
import android.graphics.*
import android.hardware.camera2.*
import android.media.ImageReader
import android.media.MediaRecorder
import android.opengl.GLES11Ext
import android.opengl.GLES20
import android.os.Environment
import android.os.Handler
import android.os.HandlerThread
import android.util.Log
import android.util.Size
import android.util.SparseArray
import android.util.SparseIntArray
import android.view.Surface
import android.view.TextureView
import androidx.lifecycle.MutableLiveData
import com.pichu.nanamare.camera.filter.mx.*
import com.pichu.nanamare.camera.filter.shader.*
import com.pichu.nanamare.custom.AutoFitTextureView
import com.pichu.nanamare.utils.CompareSizesByArea
import com.pichu.nanamare.utils.Logger
import io.reactivex.subjects.PublishSubject
import org.jetbrains.anko.windowManager
import java.io.File
import java.io.IOException
import java.text.SimpleDateFormat
import java.util.*
import java.util.concurrent.Semaphore
import java.util.concurrent.TimeUnit
import javax.microedition.khronos.egl.EGL10
import javax.microedition.khronos.egl.EGLConfig
import javax.microedition.khronos.egl.EGLContext
import javax.microedition.khronos.egl.EGLSurface
import kotlin.math.floor
import kotlin.math.max

class CameraRenderer(
    private val context: Context,
    private val tvSurface: AutoFitTextureView,
    private val liveCameraFilter: MutableLiveData<CameraFilter>,
    private val liveCameraFilters: MutableLiveData<SparseArray<CameraFilter>>,
    private val liveFilterId: MutableLiveData<Int>
) : Runnable, TextureView.SurfaceTextureListener {

    private var flashOn = false

    /**
     * OpenGLS
     */
    private val renderThread by lazy { Thread(this) }
    private var glSurfaceTexture: SurfaceTexture? = null
    private var cameraWidth: Int = 0
    private var cameraHeight: Int = 0

    private val egl10: EGL10 by lazy { EGLContext.getEGL() as EGL10 }
    private var cameraSurfaceTexture: SurfaceTexture? = null
    private val eglDisplay by lazy { egl10.eglGetDisplay(EGL10.EGL_DEFAULT_DISPLAY) }
    private var eglSurface: EGLSurface? = null
    private var eglContext: EGLContext? = null

    private var cameraTextureId: Int = 0

    var defaultUserSelectRatio = FULL_SCREEN_RATIO

    /**
     * Output file for video
     */
    private var nextVideoAbsolutePath: String? = null

    private var mediaRecorder: MediaRecorder? = null

    /**
     * Whether the app is recording video now
     */
    private var isRecordingVideo = false

    /**
     * A [Semaphore] to prevent the app from exiting before closing the camera.
     */
    private val cameraOpenCloseLock = Semaphore(1)

    /**
     * An [ImageReader] that handles still image capture.
     */
    private var imageReader: ImageReader? = null

    /**
     * This is the output file for our picture.
     */
    private lateinit var file: File

    var imageObservable = PublishSubject.create<Unit>()

    /**
     * This a callback object for the [ImageReader]. "onImageAvailable" will be called when a
     * still image is ready to be saved.
     */
    private val onImageAvailableListener = ImageReader.OnImageAvailableListener {
        backgroundHandler?.post {
            imageObservable.onNext(Unit)
        }
    }

    /**
     * An additional thread for running tasks that shouldn't block the UI.
     */
    private var backgroundThread: HandlerThread? = null

    /**
     * A [Handler] for running tasks in the background.
     */
    private var backgroundHandler: Handler? = null

    /**
     * Orientation of the camera sensor
     */
    private var sensorOrientation = 0

    /**
     * The [android.util.Size] of camera preview.
     */
    private lateinit var previewSize: Size

    /**
     * Whether the current camera device supports Flash or not.
     */
    private var flashSupported = false

    /**
     * A reference to the opened [CameraDevice].
     */
    private var cameraDevice: CameraDevice? = null

    /**
     * [CaptureRequest.Builder] for the camera preview
     */
    private lateinit var previewRequestBuilder: CaptureRequest.Builder

    /**
     * A [CameraCaptureSession] for camera preview.
     */
    private var captureSession: CameraCaptureSession? = null

    /**
     * [CaptureRequest] generated by [.previewRequestBuilder]
     */
    private lateinit var previewRequest: CaptureRequest

    /**
     * The current state of camera state for taking pictures.
     *
     * @see .captureCallback
     */
    private var state = STATE_PREVIEW

    override fun onSurfaceTextureUpdated(surface: SurfaceTexture) {}

    override fun onSurfaceTextureSizeChanged(surface: SurfaceTexture?, width: Int, height: Int) {
        cameraWidth = -width
        cameraHeight = -height
    }

    override fun onSurfaceTextureDestroyed(surface: SurfaceTexture): Boolean {
        if (renderThread.isAlive) {
            renderThread.interrupt()
        }
        CameraFilter.release()
        return true
    }

    override fun onSurfaceTextureAvailable(surface: SurfaceTexture, width: Int, height: Int) {
        openCamera(width, height)
        startRenderThread(surface, width, height)
    }

    fun startRenderThread(
        surface: SurfaceTexture,
        width: Int,
        height: Int
    ) {
        if (renderThread.isAlive) {
            renderThread.interrupt()
        }
        glSurfaceTexture = surface
        cameraWidth = -width
        cameraHeight = -height
        renderThread.start()
    }

    /**
     * [CameraDevice.StateCallback] is called when [CameraDevice] changes its state.
     */
    private val stateCallback = object : CameraDevice.StateCallback() {
        override fun onOpened(cameraDevice: CameraDevice) {
            cameraOpenCloseLock.release()
            this@CameraRenderer.cameraDevice = cameraDevice
            cameraSurfaceTexture?.let {
                backgroundHandler?.post {
                    createCameraPreviewSession(it)
                }
            }
        }

        override fun onDisconnected(cameraDevice: CameraDevice) {
            cameraOpenCloseLock.release()
            cameraDevice.close()
            this@CameraRenderer.cameraDevice = null
        }

        override fun onError(cameraDevice: CameraDevice, error: Int) {
            cameraOpenCloseLock.release()
            onDisconnected(cameraDevice)
        }
    }

    /**
     * Creates a new [CameraCaptureSession] for camera preview.
     */
    private fun createCameraPreviewSession(cameraSurfaceTexture: SurfaceTexture?) {
        try {
            val texture = cameraSurfaceTexture
            // We configure the size of default buffer to be the size of camera preview we want.
            texture?.setDefaultBufferSize(previewSize.width, previewSize.height)
            // This is the output Surface we need to start preview.
            val surface = Surface(texture)
            // We set up a CaptureRequest.Builder with the output Surface.
            previewRequestBuilder = cameraDevice!!.createCaptureRequest(
                CameraDevice.TEMPLATE_PREVIEW
            )
            previewRequestBuilder.addTarget(surface)
            // Here, we create a CameraCaptureSession for camera preview.
            cameraDevice?.createCaptureSession(
                listOf(surface, imageReader?.surface),
                object : CameraCaptureSession.StateCallback() {
                    override fun onConfigured(cameraCaptureSession: CameraCaptureSession) {
                        // The camera is already closed
                        if (cameraDevice == null) return
                        // When the session is ready, we start displaying the preview.
                        captureSession = cameraCaptureSession
                        try {
                            // Auto focus should be continuous for camera preview.
                            previewRequestBuilder.set(
                                CaptureRequest.CONTROL_AF_MODE,
                                CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE
                            )
                            // Flash is automatically enabled when necessary.
                            // setAutoFlash(previewRequestBuilder)
                            // Finally, we start displaying the camera preview.
                            previewRequest = previewRequestBuilder.build()
                            captureSession?.setRepeatingRequest(
                                previewRequest,
                                captureCallback, backgroundHandler
                            )
                        } catch (e: CameraAccessException) {
                            Log.e(TAG, e.toString())
                        }
                    }

                    override fun onConfigureFailed(session: CameraCaptureSession) {}
                }, null
            )
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        }

    }

    /**
     * A [CameraCaptureSession.CaptureCallback] that handles events related to JPEG capture.
     */
    private val captureCallback = object : CameraCaptureSession.CaptureCallback() {

        private fun process(result: CaptureResult) {
            when (state) {
                STATE_PREVIEW -> Unit // Do nothing when the camera preview is working normally.
                STATE_WAITING_LOCK -> capturePicture(result)
                STATE_WAITING_PRECAPTURE -> {
                    // CONTROL_AE_STATE can be null on some devices
                    val aeState = result.get(CaptureResult.CONTROL_AE_STATE)
                    if (aeState == null ||
                        aeState == CaptureResult.CONTROL_AE_STATE_PRECAPTURE ||
                        aeState == CaptureRequest.CONTROL_AE_STATE_FLASH_REQUIRED
                    ) {
                        state = STATE_WAITING_NON_PRECAPTURE
                    }
                }
                STATE_WAITING_NON_PRECAPTURE -> {
                    // CONTROL_AE_STATE can be null on some devices
                    val aeState = result.get(CaptureResult.CONTROL_AE_STATE)
                    if (aeState == null || aeState != CaptureResult.CONTROL_AE_STATE_PRECAPTURE) {
                        state = STATE_PICTURE_TAKEN
                        captureStillPicture()
                    }
                }
            }
        }

        private fun capturePicture(result: CaptureResult) {
            val afState = result.get(CaptureResult.CONTROL_AF_STATE)
            if (afState == null) {
                captureStillPicture()
            } else if (afState == CaptureResult.CONTROL_AF_STATE_FOCUSED_LOCKED
                || afState == CaptureResult.CONTROL_AF_STATE_NOT_FOCUSED_LOCKED
                || afState == CaptureResult.CONTROL_AF_STATE_INACTIVE
            ) {
                // CONTROL_AE_STATE can be null on some devices
                val aeState = result.get(CaptureResult.CONTROL_AE_STATE)
                if (aeState == null || aeState == CaptureResult.CONTROL_AE_STATE_CONVERGED) {
                    state = STATE_PICTURE_TAKEN
                    captureStillPicture()
                } else {
                    runPreCaptureSequence()
                }
            }
        }

        override fun onCaptureProgressed(
            session: CameraCaptureSession,
            request: CaptureRequest,
            partialResult: CaptureResult
        ) {
            process(partialResult)
        }

        override fun onCaptureCompleted(
            session: CameraCaptureSession,
            request: CaptureRequest,
            result: TotalCaptureResult
        ) {
            process(result)
        }

    }

    /**
     * Run the precapture sequence for capturing a still image. This method should be called when
     * we get a response in [.captureCallback] from [.lockFocus].
     */
    private fun runPreCaptureSequence() {
        try {
            // This is how to tell the camera to trigger.
            previewRequestBuilder.set(
                CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER,
                CaptureRequest.CONTROL_AE_PRECAPTURE_TRIGGER_START
            )
            // Tell #captureCallback to wait for the precapture sequence to be set.
            state = STATE_WAITING_PRECAPTURE
            captureSession?.capture(
                previewRequestBuilder.build(), captureCallback,
                backgroundHandler
            )
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        }

    }

    /**
     * Capture a still picture. This method should be called when we get a response in
     * [.captureCallback] from both [.lockFocus].
     */
    private fun captureStillPicture() {
        try {
            if (cameraDevice == null) return
            val rotation = context.windowManager.defaultDisplay.rotation
            // This is the CaptureRequest.Builder that we use to take a picture.
            val captureBuilder = cameraDevice?.createCaptureRequest(
                CameraDevice.TEMPLATE_STILL_CAPTURE
            )?.apply {
                addTarget(imageReader?.surface!!)
                // Sensor orientation is 90 for most devices, or 270 for some devices (eg. Nexus 5X)
                // We have to take that into account and rotate JPEG properly.
                // For devices with orientation of 90, we return our mapping from ORIENTATIONS.
                // For devices with orientation of 270, we need to rotate the JPEG 180 degrees.
                set(
                    CaptureRequest.JPEG_ORIENTATION,
                    (ORIENTATIONS.get(rotation) + sensorOrientation + 270) % 360
                )
                // Use the same AE and AF modes as the preview.
                set(
                    CaptureRequest.CONTROL_AF_MODE,
                    CaptureRequest.CONTROL_AF_MODE_CONTINUOUS_PICTURE
                )
            }?.also {
                // Flash is automatically enabled when necessary.
                // setAutoFlash(it)
            }

            val captureCallback = object : CameraCaptureSession.CaptureCallback() {
                override fun onCaptureCompleted(
                    session: CameraCaptureSession,
                    request: CaptureRequest,
                    result: TotalCaptureResult
                ) {
                    unlockFocus()
                }
            }

            captureSession?.apply {
                stopRepeating()
                abortCaptures()
                capture(captureBuilder?.build()!!, captureCallback, null)
            }
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        }

    }

    /**
     * Unlock the focus. This method should be called when still image capture sequence is
     * finished.
     */
    private fun unlockFocus() {
        try {
            // Reset the auto-focus trigger
            previewRequestBuilder.set(
                CaptureRequest.CONTROL_AF_TRIGGER,
                CameraMetadata.CONTROL_AF_TRIGGER_CANCEL
            )
            // Flash is automatically enabled when necessary.
            // setAutoFlash(previewRequestBuilder)
            captureSession?.capture(
                previewRequestBuilder.build(), captureCallback,
                backgroundHandler
            )
            // After this, the camera will go back to the normal state of preview.
            state = STATE_PREVIEW
            captureSession?.setRepeatingRequest(
                previewRequest, captureCallback,
                backgroundHandler
            )
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        }

    }

    // TODO 29 api change save file
    fun lockFocus() {
        try {
            val filePath =
                Environment.getExternalStoragePublicDirectory(Environment.DIRECTORY_PICTURES)
                    .toString() + "/Pichu"
            File(filePath).let {
                if (!it.exists()) {
                    it.mkdir()
                }
            }
            file = File(
                filePath,
                SimpleDateFormat("yyyyMMdd_HHmmss", Locale.getDefault()).format(Date()) + ".jpg"
            )

            // This is how to tell the camera to lock focus.
            previewRequestBuilder.set(
                CaptureRequest.CONTROL_AF_TRIGGER,
                CameraMetadata.CONTROL_AF_TRIGGER_START
            )
            // Tell #captureCallback to wait for the lock.
            state = STATE_WAITING_LOCK
            captureSession?.capture(
                previewRequestBuilder.build(), captureCallback,
                backgroundHandler
            )
        } catch (e: CameraAccessException) {
            Logger.e(TAG, e)
        }

    }

    private fun setAutoFlash(requestBuilder: CaptureRequest.Builder) {
        if (flashSupported) {
            requestBuilder.set(
                CaptureRequest.CONTROL_AE_MODE,
                CaptureRequest.CONTROL_AE_MODE_ON_AUTO_FLASH
            )
        }
    }

    /**
     * Opens the camera specified by [CameraRenderer.CAMERA_ID].
     */
    @SuppressLint("MissingPermission")
    fun openCamera(width: Int, height: Int) {
        setUpCameraOutputs(width, height)
        configureTransform(width, height)
        mediaRecorder = MediaRecorder()
        val manager = context.getSystemService(Context.CAMERA_SERVICE) as CameraManager
        try {
            // Wait for camera to open - 2.5 seconds is sufficient
            if (!cameraOpenCloseLock.tryAcquire(2500, TimeUnit.MILLISECONDS)) {
                throw RuntimeException("Time out waiting to lock camera opening.")
            }
            manager.openCamera(CAMERA_ID, stateCallback, backgroundHandler)
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        } catch (e: InterruptedException) {
            throw RuntimeException("Interrupted while trying to lock camera opening.", e)
        }

    }

    /**
     * Sets up member variables related to camera.
     *
     * @param width  The width of available size for camera preview
     * @param height The height of available size for camera preview
     */
    private fun setUpCameraOutputs(width: Int, height: Int) {
        val manager = context.getSystemService(Context.CAMERA_SERVICE) as CameraManager
        try {
            for (cameraId in manager.cameraIdList) {
                val characteristics = manager.getCameraCharacteristics(cameraId)

                val map = characteristics.get(
                    CameraCharacteristics.SCALER_STREAM_CONFIGURATION_MAP
                ) ?: continue

                // For still image captures, we use the largest available size.
                val largest = Collections.max(
                    listOf(*map.getOutputSizes(ImageFormat.YUV_420_888)),
                    CompareSizesByArea()
                )
                imageReader = ImageReader.newInstance(
                    largest.width, largest.height,
                    ImageFormat.YUV_420_888, /*maxImages*/ 2
                ).apply {
                    setOnImageAvailableListener(onImageAvailableListener, backgroundHandler)
                }

                // Find out if we need to swap dimension to get the preview size relative to sensor
                // coordinate.
                val displayRotation = context.windowManager.defaultDisplay.rotation

                sensorOrientation = characteristics.get(CameraCharacteristics.SENSOR_ORIENTATION)!!
                val swappedDimensions = areDimensionsSwapped(displayRotation)

                val displaySize = Point()
                context.windowManager.defaultDisplay.getSize(displaySize)
                val rotatedPreviewWidth = if (swappedDimensions) height else width
                val rotatedPreviewHeight = if (swappedDimensions) width else height
                var maxPreviewWidth = if (swappedDimensions) displaySize.y else displaySize.x
                var maxPreviewHeight = if (swappedDimensions) displaySize.x else displaySize.y

                if (maxPreviewWidth > MAX_PREVIEW_WIDTH) maxPreviewWidth = MAX_PREVIEW_WIDTH
                if (maxPreviewHeight > MAX_PREVIEW_HEIGHT) maxPreviewHeight = MAX_PREVIEW_HEIGHT

                // Danger, W.R.! Attempting to use too large a preview size could exceed the camera
                // bus' bandwidth limitation, resulting in gorgeous previews but the storage of
                // garbage capture data.
                previewSize = chooseOptimalSize(
                    map.getOutputSizes(SurfaceTexture::class.java),
                    defaultUserSelectRatio
                )

                // We fit the aspect ratio of TextureView to the size of preview we picked.
                if (context.resources.configuration.orientation == Configuration.ORIENTATION_LANDSCAPE) {
                    tvSurface.setAspectRatio(previewSize.width, previewSize.height)
                } else {
                    tvSurface.setAspectRatio(previewSize.height, previewSize.width)
                }

                // Check if the flash is supported.
                flashSupported =
                    characteristics.get(CameraCharacteristics.FLASH_INFO_AVAILABLE) == true

                // We've found a viable camera and finished setting up member variables,
                // so we don't need to iterate through other available cameras.
                return
            }
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        } catch (e: NullPointerException) {
            // Currently an NPE is thrown when the Camera2API is used but not supported on the
            // device this code runs.

        }

    }

    /**
     * Configures the necessary [android.graphics.Matrix] transformation to `textureView`.
     * This method should be called after the camera preview size is determined in
     * setUpCameraOutputs and also the size of `textureView` is fixed.
     *
     * @param viewWidth  The width of `textureView`
     * @param viewHeight The height of `textureView`
     */
    private fun configureTransform(viewWidth: Int, viewHeight: Int) {
        val rotation = context.windowManager.defaultDisplay.rotation
        val matrix = Matrix()
        val viewRect = RectF(0f, 0f, viewWidth.toFloat(), viewHeight.toFloat())
        val bufferRect = RectF(0f, 0f, previewSize.height.toFloat(), previewSize.width.toFloat())
        val centerX = viewRect.centerX()
        val centerY = viewRect.centerY()

        if (Surface.ROTATION_90 == rotation || Surface.ROTATION_270 == rotation) {
            bufferRect.offset(centerX - bufferRect.centerX(), centerY - bufferRect.centerY())
            val scale = max(
                viewHeight.toFloat() / previewSize.height,
                viewWidth.toFloat() / previewSize.width
            )
            with(matrix) {
                setRectToRect(viewRect, bufferRect, Matrix.ScaleToFit.FILL)
                postScale(scale, scale, centerX, centerY)
                postRotate((90 * (rotation - 2)).toFloat(), centerX, centerY)
            }
        } else if (Surface.ROTATION_180 == rotation) {
            matrix.postRotate(180f, centerX, centerY)
        }
        tvSurface.setTransform(matrix)
    }

    /**
     * Determines if the dimensions are swapped given the phone's current rotation.
     *
     * @param displayRotation The current rotation of the display
     *
     * @return true if the dimensions are swapped, false otherwise.
     */
    private fun areDimensionsSwapped(displayRotation: Int): Boolean {
        var swappedDimensions = false
        when (displayRotation) {
            Surface.ROTATION_0, Surface.ROTATION_180 -> {
                if (sensorOrientation == 90 || sensorOrientation == 270) {
                    swappedDimensions = true
                }
            }
            Surface.ROTATION_90, Surface.ROTATION_270 -> {
                if (sensorOrientation == 0 || sensorOrientation == 180) {
                    swappedDimensions = true
                }
            }
            else -> {
                Log.e(TAG, "Display rotation is invalid: $displayRotation")
            }
        }
        return swappedDimensions
    }

    fun setSelectedFilter(id: Int) {
        liveCameraFilter.postValue(liveCameraFilters.value?.get(id))
        liveCameraFilter.value?.let {
            it.onAttach()
        }
    }

    // TODO to apply design pattern
    override fun run() {
        initGL(glSurfaceTexture)

        // Setup camera filters map
        liveCameraFilters.value?.append(
            0,
            OriginalFilter(context, "Original")
        )
        liveCameraFilters.value?.append(
            1,
            EdgeDetectionFilter(context, "EdgeDetection")
        )
        liveCameraFilters.value?.append(
            2,
            PixelizeFilter(context, "Pixelize")
        )
        liveCameraFilters.value?.append(
            3,
            EMInterferenceFilter(context, "EMInterference")
        )
        liveCameraFilters.value?.append(
            4,
            TrianglesMosaicFilter(
                context,
                "TrianglesMosaic"
            )
        )
        liveCameraFilters.value?.append(
            5,
            LegofiedFilter(context, "Legofied")
        )
        liveCameraFilters.value?.append(
            6,
            TileMosaicFilter(context, "TileMosaic")
        )
        liveCameraFilters.value?.append(
            7,
            BlueOrangeFilter(context, "BlueOrange")
        )
        liveCameraFilters.value?.append(
            8,
            ChromaticAberrationFilter(
                context,
                "ChromaticAberration"
            )
        )
        liveCameraFilters.value?.append(
            9,
            BasicDeformFilter(context, "BasicDeform")
        )
        liveCameraFilters.value?.append(
            10,
            ContrastFilter(context, "Contrast")
        )
        liveCameraFilters.value?.append(
            11,
            NoiseWarpFilter(context, "NoiseWarp")
        )
        liveCameraFilters.value?.append(
            12,
            RefractionFilter(context, "Refraction")
        )
        liveCameraFilters.value?.append(
            13,
            MappingFilter(context, "Mapping")
        )
        liveCameraFilters.value?.append(
            14,
            CrosshatchFilter(context, "Crosshatch")
        )
        liveCameraFilters.value?.append(
            15,
            LichtensteinEsqueFilter(
                context,
                "LichtensteinEsque"
            )
        )
        liveCameraFilters.value?.append(
            16,
            AsciiArtFilter(context, "AsciiArt")
        )
        liveCameraFilters.value?.append(
            17,
            MoneyFilter(context, "Money")
        )
        liveCameraFilters.value?.append(
            18,
            CrackedFilter(context, "Cracked")
        )
        liveCameraFilters.value?.append(
            19,
            PolygonizationFilter(context, "Polygonization")
        )
        liveCameraFilters.value?.append(
            20,
            JFAVoronoiFilter(context, "JFAVoronoi")
        )
        liveCameraFilters.value?.append(
            21,
            ColorBlindFilter(context, "FrozenGlass")
        )
        liveCameraFilters.value?.append(
            22,
            NightmareFilter(context, "Nightmare")
        )
        liveCameraFilters.value?.append(
            23,
            OrangeFilter(context, "Orange")
        )
        liveCameraFilters.value?.append(
            24,
            EarlyBirdFilter(context, "EarlyBird")
        )
        liveCameraFilters.value?.append(
            25,
            BrannanFilter(context, "RainDrop")
        )
        liveCameraFilters.value?.append(
            26,
            SepiaFilter(context, "Sepia")
        )
        liveCameraFilters.value?.append(
            27,
            GrayScaleFilter(context, "Grayscale")
        )
        liveCameraFilters.value?.append(
            28,
            WhiteFilter(context, "White")
        )
        liveCameraFilters.value?.append(
            29,
            WaveFilter(context, "Wave")
        )

        liveCameraFilters.value?.append(
            30,
            MxBrightnessFilter(context, "MxBrightness")
        )
        liveCameraFilters.value?.append(
            31,
            MxFaceBeautyFilter(context, "MxFaceBeauty")
        )
        liveCameraFilters.value?.append(
            32,
            MxFillLightFilter(context, "MxFillLight")
        )
        liveCameraFilters.value?.append(
            33,
            MxGreenHouseFilter(context, "MxGreenHouse")
        )
        liveCameraFilters.value?.append(
            34,
            MxLomoFilter(context, "MxLomo")
        )
        liveCameraFilters.value?.append(
            35,
            MxMultiplyFilter(context, "MxMultiply")
        )
        liveCameraFilters.value?.append(
            36,
            MxPastTimeFilter(context, "MxPastTime")
        )
        liveCameraFilters.value?.append(
            37,
            MxPrintingFilter(context, "MxPrinting")
        )
        liveCameraFilters.value?.append(
            38,
            MxProFilter(context, "MxPro")
        )
        liveCameraFilters.value?.append(
            39,
            MxReminiscenceFilter(context, "MxReminiscence")
        )
        liveCameraFilters.value?.append(
            40,
            MxShiftColorFilter(context, "MxShiftColor")
        )
        liveCameraFilters.value?.append(
            41,
            MxSunnyFilter(context, "MxSunny")
        )
        liveCameraFilters.value?.append(
            42,
            MxToyFilter(context, "MxToy")
        )
        liveCameraFilters.value?.append(
            43,
            MxVignetteFilter(context, "MxVignette")
        )
        liveCameraFilters.value?.append(
            44,
            MxMoonLightFilter(context, "MxMoonLight")
        )
        liveCameraFilters.value?.append(
            45,
            GuassianBlurFilter(
                context,
                "GuassianBlur"
            )
        )

        setSelectedFilter(liveFilterId.value!!)

        // Create texture for camera preview
        cameraTextureId = MyGLUtils.genTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES)
        cameraSurfaceTexture = SurfaceTexture(cameraTextureId)

        backgroundHandler?.post {
            // 카메라는 열렸고, GLS 초기화 끝났으니 Preview session 요청하기
            createCameraPreviewSession(cameraSurfaceTexture)
        }

        // Render loop
        while (!Thread.currentThread().isInterrupted) {
            try {
                if (cameraWidth < 0 && cameraHeight < 0) {
                    cameraWidth = -cameraWidth
                    cameraHeight = -cameraHeight
                    GLES20.glViewport(0, 0, cameraWidth, cameraHeight)
                }
                GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT)
                // Update the camera preview texture
                synchronized(this) {
                    Logger.d(TAG, "Update Image")
                    cameraSurfaceTexture?.updateTexImage()
                }
                // Draw camera preview
                liveCameraFilter.value?.draw(cameraTextureId, cameraWidth, cameraHeight)
                // Flush
                GLES20.glFlush()
                egl10.eglSwapBuffers(eglDisplay, eglSurface)
                // Thread.sleep(DRAW_INTERVAL.toLong())
            } catch (e: InterruptedException) {
                Thread.currentThread().interrupt()
            }
        }

        cameraSurfaceTexture?.release()
        GLES20.glDeleteTextures(1, intArrayOf(cameraTextureId), 0)
    }

    private fun initGL(texture: SurfaceTexture?) {
        if (eglDisplay === EGL10.EGL_NO_DISPLAY) {
            throw RuntimeException(
                "eglGetDisplay failed " + android.opengl.GLUtils.getEGLErrorString(
                    egl10.eglGetError()
                )
            )
        }
        val version = IntArray(2)
        if (!egl10.eglInitialize(eglDisplay, version)) {
            throw RuntimeException(
                "eglInitialize failed " + android.opengl.GLUtils.getEGLErrorString(
                    egl10.eglGetError()
                )
            )
        }
        val configsCount = IntArray(1)
        val configs = arrayOfNulls<EGLConfig>(1)
        val configSpec = intArrayOf(
            EGL10.EGL_RENDERABLE_TYPE,
            EGL_OPEN_GL_ES2_BIT,
            EGL10.EGL_RED_SIZE,
            8,
            EGL10.EGL_GREEN_SIZE,
            8,
            EGL10.EGL_BLUE_SIZE,
            8,
            EGL10.EGL_ALPHA_SIZE,
            8,
            EGL10.EGL_DEPTH_SIZE,
            0,
            EGL10.EGL_STENCIL_SIZE,
            0,
            EGL10.EGL_NONE
        )

        var eglConfig: EGLConfig? = null
        if (!egl10.eglChooseConfig(eglDisplay, configSpec, configs, 1, configsCount)) {
            throw IllegalArgumentException(
                "eglChooseConfig failed " + android.opengl.GLUtils.getEGLErrorString(
                    egl10.eglGetError()
                )
            )
        } else if (configsCount[0] > 0) {
            eglConfig = configs[0]
        }
        if (eglConfig == null) {
            throw RuntimeException("eglConfig not initialized")
        }

        val attrib = intArrayOf(EGL_CONTEXT_CLIENT_VERSION, 2, EGL10.EGL_NONE)
        eglContext =
            egl10.eglCreateContext(eglDisplay, eglConfig, EGL10.EGL_NO_CONTEXT, attrib)
        eglSurface = egl10.eglCreateWindowSurface(eglDisplay, eglConfig, texture, null)

        if (eglSurface == null || eglSurface == EGL10.EGL_NO_SURFACE) {
            val error = egl10.eglGetError()
            if (error == EGL10.EGL_BAD_NATIVE_WINDOW) {
                Log.e(TAG, "eglCreateWindowSurface returned EGL10.EGL_BAD_NATIVE_WINDOW")
                return
            }
            throw RuntimeException(
                "eglCreateWindowSurface failed " + android.opengl.GLUtils.getEGLErrorString(
                    error
                )
            )
        }

        if (!egl10.eglMakeCurrent(eglDisplay, eglSurface, eglSurface, eglContext)) {
            throw RuntimeException(
                "eglMakeCurrent failed " + android.opengl.GLUtils.getEGLErrorString(
                    egl10.eglGetError()
                )
            )
        }
    }

    /**
     * Starts a background thread and its [Handler].
     */
    fun startBackgroundThread() {
        backgroundThread = HandlerThread("CameraBackground").also { it.start() }
        backgroundHandler = Handler(backgroundThread?.looper)
    }

    /**
     * Closes the current [CameraDevice].
     */
    fun closeCamera() {
        try {
            cameraOpenCloseLock.acquire()
            captureSession?.close()
            captureSession = null
            cameraDevice?.close()
            cameraDevice = null
            imageReader?.close()
            imageReader = null
        } catch (e: InterruptedException) {
            throw RuntimeException("Interrupted while trying to lock camera closing.", e)
        } finally {
            cameraOpenCloseLock.release()
        }
    }

    /**
     * Stops the background thread and its [Handler].
     */
    fun stopBackgroundThread() {
        backgroundThread?.quitSafely()
        try {
            backgroundThread?.join()
            backgroundThread = null
            backgroundHandler = null
        } catch (e: InterruptedException) {
            Log.e(TAG, e.toString())
        }
    }

    fun onTurnOnFlash() {
        try {
            if (CAMERA_ID == CAMERA_BACK) {
                if (flashSupported) {
                    flashOn = if (flashOn) {
                        previewRequestBuilder.set(
                            CaptureRequest.FLASH_MODE,
                            CaptureRequest.FLASH_MODE_OFF
                        )
                        captureSession?.setRepeatingRequest(
                            previewRequestBuilder.build(),
                            null,
                            null
                        )
                        false
                    } else {
                        previewRequestBuilder.set(
                            CaptureRequest.FLASH_MODE,
                            CaptureRequest.FLASH_MODE_TORCH
                        )
                        captureSession?.setRepeatingRequest(
                            previewRequestBuilder.build(),
                            null,
                            null
                        )
                        true
                    }
                }
            }
        } catch (e: CameraAccessException) {
            e.printStackTrace()
        }
    }

    fun onSwitchCamera() {
        CAMERA_ID = if (CAMERA_ID == CAMERA_FRONT) {
            CAMERA_BACK
        } else {
            CAMERA_FRONT
        }
        closeCamera()
        reopenCamera()
    }

    fun reopenCamera() {
        if (tvSurface.isAvailable) {
            openCamera(tvSurface.width, tvSurface.height)
        } else {
            tvSurface.surfaceTextureListener = this
        }
    }

    fun startRecordingVideo() {
        if (cameraDevice == null || !tvSurface.isAvailable) return

        try {
            closePreviewSession()
            setUpMediaRecorder()

            // Set up Surface for camera preview and MediaRecorder
            // val previewSurface = Surface(tvSurface.surfaceTexture)
            val recorderSurface = mediaRecorder!!.surface
            val surfaces = ArrayList<Surface>().apply {
                add(recorderSurface)
                // add(previewSurface)
            }
            previewRequestBuilder =
                cameraDevice!!.createCaptureRequest(CameraDevice.TEMPLATE_RECORD).apply {
                    addTarget(recorderSurface)
                    // addTarget(previewSurface)
                }

            // Start a capture session
            // Once the session starts, we can update the UI and start recording
            cameraDevice?.createCaptureSession(
                surfaces,
                object : CameraCaptureSession.StateCallback() {
                    override fun onConfigured(cameraCaptureSession: CameraCaptureSession) {
                        captureSession = cameraCaptureSession
                        updatePreview()
                        isRecordingVideo = true
                        mediaRecorder?.start()

                    }

                    override fun onConfigureFailed(cameraCaptureSession: CameraCaptureSession) {
                        Logger.d(TAG, "onConfigureFailed")
                    }
                }, backgroundHandler
            )
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        } catch (e: IOException) {
            Log.e(TAG, e.toString())
        }

    }

    fun stopRecordingVideo() {
        isRecordingVideo = false
        mediaRecorder?.apply {
            stop()
            reset()
        }
        Logger.d(TAG, "Video saved: $nextVideoAbsolutePath")
        nextVideoAbsolutePath = null
        // startPreview()
        cameraSurfaceTexture?.let {
            backgroundHandler?.post {
                createCameraPreviewSession(it)
            }
        }
    }

    /**
     * Start the camera preview.
     */
    private fun startPreview() {
        if (cameraDevice == null || !tvSurface.isAvailable) return

        try {
            closePreviewSession()
            val texture = tvSurface.surfaceTexture
            texture.setDefaultBufferSize(previewSize.width, previewSize.height)
            previewRequestBuilder =
                cameraDevice!!.createCaptureRequest(CameraDevice.TEMPLATE_PREVIEW)

            val previewSurface = Surface(texture)
            previewRequestBuilder.addTarget(previewSurface)

            cameraDevice?.createCaptureSession(
                listOf(previewSurface),
                object : CameraCaptureSession.StateCallback() {

                    override fun onConfigured(session: CameraCaptureSession) {
                        captureSession = session
                        updatePreview()
                    }

                    override fun onConfigureFailed(session: CameraCaptureSession) {
                        Logger.d(TAG, "onConfigureFailed")
                    }
                }, backgroundHandler
            )
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        }

    }

    /**
     * Update the camera preview. [startPreview] needs to be called in advance.
     */
    private fun updatePreview() {
        if (cameraDevice == null) return

        try {
            setUpCaptureRequestBuilder(previewRequestBuilder)
            HandlerThread("CameraPreview").start()
            captureSession?.setRepeatingRequest(
                previewRequestBuilder.build(),
                null, backgroundHandler
            )
        } catch (e: CameraAccessException) {
            Log.e(TAG, e.toString())
        }

    }

    private fun setUpCaptureRequestBuilder(builder: CaptureRequest.Builder?) {
        builder?.set(CaptureRequest.CONTROL_MODE, CameraMetadata.CONTROL_MODE_AUTO)
    }

    @Throws(IOException::class)
    private fun setUpMediaRecorder() {

        if (nextVideoAbsolutePath.isNullOrEmpty()) {
            nextVideoAbsolutePath = getVideoFilePath(context)
        }

        val rotation = context.windowManager.defaultDisplay.rotation
        when (sensorOrientation) {
            SENSOR_ORIENTATION_DEFAULT_DEGREES ->
                mediaRecorder?.setOrientationHint(DEFAULT_ORIENTATIONS.get(rotation))
            SENSOR_ORIENTATION_INVERSE_DEGREES ->
                mediaRecorder?.setOrientationHint(INVERSE_ORIENTATIONS.get(rotation))
        }

        mediaRecorder?.apply {
            // AVD disabled Audio
            // setAudioSource(MediaRecorder.AudioSource.MIC)
            // setAudioEncoder(MediaRecorder.AudioEncoder.AAC)
            setVideoSource(MediaRecorder.VideoSource.SURFACE)
            setOutputFormat(MediaRecorder.OutputFormat.MPEG_4)
            setOutputFile(nextVideoAbsolutePath)
            setVideoEncodingBitRate(10000000)
            setVideoFrameRate(30)
            setVideoSize(previewSize.width, previewSize.height)
            setVideoEncoder(MediaRecorder.VideoEncoder.H264)
            prepare()
        }
    }

    private fun getVideoFilePath(context: Context?): String {
        val filename = "${System.currentTimeMillis()}.mp4"
        val dir = context?.getExternalFilesDir(null)

        return if (dir == null) {
            filename
        } else {
            "${dir.absolutePath}/$filename"
        }
    }

    private fun closePreviewSession() {
        captureSession?.close()
        captureSession = null
    }

    companion object {
        @JvmStatic
        val TAG = CameraRenderer::class::java.name
        private const val EGL_OPEN_GL_ES2_BIT = 4
        private const val EGL_CONTEXT_CLIENT_VERSION = 0x3098
        private const val DRAW_INTERVAL = 1000 / 30

        /**
         * Conversion from screen rotation to JPEG orientation.
         */
        private val ORIENTATIONS = SparseIntArray()

        init {
            ORIENTATIONS.append(Surface.ROTATION_0, 90)
            ORIENTATIONS.append(Surface.ROTATION_90, 0)
            ORIENTATIONS.append(Surface.ROTATION_180, 270)
            ORIENTATIONS.append(Surface.ROTATION_270, 180)
        }

        private val DEFAULT_ORIENTATIONS = SparseIntArray().apply {
            append(Surface.ROTATION_0, 90)
            append(Surface.ROTATION_90, 0)
            append(Surface.ROTATION_180, 270)
            append(Surface.ROTATION_270, 180)
        }
        private val INVERSE_ORIENTATIONS = SparseIntArray().apply {
            append(Surface.ROTATION_0, 270)
            append(Surface.ROTATION_90, 180)
            append(Surface.ROTATION_180, 90)
            append(Surface.ROTATION_270, 0)
        }

        /**
         * Camera state: Showing camera preview.
         */
        private const val STATE_PREVIEW = 0

        /**
         * Camera state: Waiting for the focus to be locked.
         */
        private const val STATE_WAITING_LOCK = 1

        /**
         * Camera state: Waiting for the exposure to be precapture state.
         */
        private const val STATE_WAITING_PRECAPTURE = 2

        /**
         * Camera state: Waiting for the exposure state to be something other than precapture.
         */
        private const val STATE_WAITING_NON_PRECAPTURE = 3

        /**
         * Camera state: Picture was taken.
         */
        private const val STATE_PICTURE_TAKEN = 4

        /**
         * Max preview width that is guaranteed by Camera2 API
         */
        private const val MAX_PREVIEW_WIDTH = 3200

        /**
         * Max preview height that is guaranteed by Camera2 API
         */
        private const val MAX_PREVIEW_HEIGHT = 1800

        private const val SENSOR_ORIENTATION_DEFAULT_DEGREES = 90
        private const val SENSOR_ORIENTATION_INVERSE_DEGREES = 270

        const val FULL_SCREEN_RATIO = 1.7

        const val NOT_FULL_SCREEN_RATIO = 1.3

        private const val CAMERA_FRONT = "1"
        private const val CAMERA_BACK = "0"

        private var CAMERA_ID = CAMERA_BACK

        /**
         * Given `choices` of `Size`s supported by a camera, choose the smallest one that
         * is at least as large as the respective texture view size, and that is at most as large as
         * the respective max size, and whose aspect ratio matches with the specified value. If such
         * size doesn't exist, choose the largest one that is at most as large as the respective max
         * size, and whose aspect ratio matches with the specified value.
         *
         * @param choices           The list of sizes that the camera supports for the intended
         *                          output class
         * @param textureViewWidth  The width of the texture view relative to sensor coordinate
         * @param textureViewHeight The height of the texture view relative to sensor coordinate
         * @param maxWidth          The maximum width that can be chosen
         * @param maxHeight         The maximum height that can be chosen
         * @param aspectRatio       The aspect ratio
         * @return The optimal `Size`, or an arbitrary one if none were big enough
         */
        @JvmStatic
        private fun chooseOptimalSize(
            choices: Array<Size>,
            defaultUserSelectRatio: Double
        ): Size {
            // Collect the supported resolutions that are at least as big as the preview Surface
            val fullScreens = ArrayList<Size>()
            // Collect the supported resolutions that are smaller than the preview Surface
            val notFullScreens = ArrayList<Size>()
            val etcScreens = ArrayList<Size>()
            for (option in choices) {

                val ratio: Double = option.width.toDouble() / option.height.toDouble()
                when {
                    floor(ratio * 10f) / 10.0f == 1.7 -> fullScreens.add(option)
                    floor(ratio * 10f) / 10.0f == 1.3 -> notFullScreens.add(option)
                    else -> etcScreens.add(option)
                }
            }

            return if (defaultUserSelectRatio == FULL_SCREEN_RATIO) {
                if (fullScreens.size > 0) {
                    Collections.max(fullScreens, CompareSizesByArea())
                } else {
                    if (notFullScreens.size > 0) {
                        Collections.max(notFullScreens, CompareSizesByArea())
                    } else {
                        if (etcScreens.size > 0) {
                            Collections.max(etcScreens, CompareSizesByArea())
                        } else {
                            choices[0]
                        }
                    }

                }
            } else if (defaultUserSelectRatio == NOT_FULL_SCREEN_RATIO) {
                if (notFullScreens.size > 0) {
                    Collections.max(notFullScreens, CompareSizesByArea())
                } else {
                    if (fullScreens.size > 0) {
                        Collections.max(fullScreens, CompareSizesByArea())
                    } else {
                        if (etcScreens.size > 0) {
                            Collections.max(etcScreens, CompareSizesByArea())
                        } else {
                            choices[0]
                        }
                    }
                }
            } else {
                if (etcScreens.size > 0) {
                    Collections.max(etcScreens, CompareSizesByArea())
                } else {
                    choices[0]
                }
            }
        }
    }
}